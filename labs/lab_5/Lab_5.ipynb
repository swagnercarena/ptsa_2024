{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filtering and Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview__: This lab is meant as an introduction to latent dynamical systems and Kalman filtering.\n",
    "\n",
    "__Goals__: Students should:\n",
    "\n",
    "1. Be able to implement Kalman filtering.\n",
    "2. Gain intuition for how different assumptions about the parameters change the distribution of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_means_and_cov(means: np.ndarray, covariances: np.ndarray, ax: Any, color: str, label: str):\n",
    "    \"\"\"Plot the mean and covariance of our filtering/smoothing.\n",
    "\n",
    "    Args:\n",
    "        means: Means to plot.\n",
    "        covariances: Covariances to plot.\n",
    "        ax: Axis on which to plot.\n",
    "        color: Color for plotting.\n",
    "        label: Label for plotted points.\n",
    "\n",
    "    Notes:\n",
    "        Will plot the 68% contours from the covariances.\n",
    "    \"\"\"\n",
    "    # Plot the trend line.\n",
    "    ax.plot(means[:,0], means[:,1], '-', color=color, label=label)\n",
    "\n",
    "    # Plot the ellipses for covariances. Assume they are diagonal (true for this lab).\n",
    "    for i in range(len(means)):\n",
    "        elip = Ellipse((means[i,0], means[i,1]), np.sqrt(covariances[i,0,0]), np.sqrt(covariances[i,1,1]), color=color, alpha=0.1)\n",
    "        ax.add_patch(elip)\n",
    "\n",
    "def plot_true_path(time: np.ndarray, ax: Any):\n",
    "    \"\"\"Plot the true trajectory to compare to the latent space.\n",
    "\n",
    "    Args:\n",
    "        time: Times at which to plot the true trajectory\n",
    "        ax: Axis on which to plot.\n",
    "    \"\"\"\n",
    "    v_true = 10.0\n",
    "    theta_true = np.pi / 3\n",
    "    x_true = np.cos(theta_true) * v_true * time\n",
    "    y_true = np.sin(theta_true) * v_true * time - 0.5 * 10.0 * time ** 2\n",
    "\n",
    "    ax.plot(x_true, y_true, color='black', label='True Trajectory')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Running a Kalman Filter / Smoothing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start from a pre-generated dataset for this lab. I will tell you that this data comes from a simple kinemtaics simulation that follows these equations:\n",
    "\n",
    "$$\n",
    "X_t = \\left[ v \\cos(\\theta) t, v \\sin(\\theta) t + \\frac{a}{2} t^2 \\right] + W_t\n",
    "$$\n",
    "\n",
    "where $v$, $a$, and $\\theta$ are parameters of our system and $W_t$ is a two-dimensional white-noise vector $W_t \\sim \\mathcal{N}(0, \\sigma_w^2 \\mathbb{I}$). This equation doesn't fully align with the assumptions of our LDS model, so it will be interesting to see how our model responds to the data. We will model the data as a linear dynamical system like the one we introduced in this week's lecture. Our latent space is the true position of our object as a function of time, and our observations are the noisy readout of those true positions.\n",
    "\n",
    "In the future, we will discuss how to learn the parameters of our model from the data, but for now let's just make reasonable choices for our parameters and conduct our filtering from there. Let's start with our equations:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_t = \\mathbf{A} \\mathbf{z}_{t-1} + \\mathbf{w}_t\n",
    "$$\n",
    "$$\n",
    "\\mathbf{x}_t = \\mathbf{C} \\mathbf{z}_{t} + \\mathbf{v}_t,\n",
    "$$\n",
    "\n",
    "and set $\\mathbf{w}_t \\sim \\mathcal{N}(0,\\sigma_w^2 \\mathbb{I})$,  $\\mathbf{v}_t \\sim \\mathcal{N}(0,\\sigma_v^2 \\mathbb{I})$,  $A = a \\mathbb{I}$, and $C = c \\mathbb{I}$. We will pick different values of $\\sigma_w, \\sigma_v, a, c$ throughout the lab. Notice that we are assuming the same dimension for the latent and observation space in this model.\n",
    "\n",
    "Let's start by implementing a class that can conduct the Kalman filtering and smoothing steps for our model. Assume that our initial state is determined by:\n",
    "$$\n",
    "\\mathbf{z}_0 \\sim \\mathcal{N}(\\mu_0, \\sigma_{w,0}^2 \\mathbb{I}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanFilter:\n",
    "    \"\"\"Class that implements the Kalman Filter for our LDS model.\n",
    "\n",
    "    Args:\n",
    "        sigma_w: Standard deviation of latent space noise.\n",
    "        sigma_v: Standard deviation of observation noise.\n",
    "        a: Magnitude of latent space transition matrix.\n",
    "        c: Magnitude of the observation matrix.\n",
    "        dim_z: Dimension of latent space.\n",
    "        dim_x: Dimension of observation space.\n",
    "        sigma_w_zero: Initial standard deviation of the zero state.\n",
    "        mu_zero: Initial mean of the zero state.\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma_w: float, sigma_v: float, a: float, c: float, dim_z: int, \n",
    "                 dim_x: int, sigma_w_zero: float, mu_zero: np.ndarray):\n",
    "        \"\"\"Initialize our class.\"\"\"\n",
    "        # Save a few variables for bookkeeping\n",
    "        self.dim_x = dim_x\n",
    "        self.dim_z = dim_z\n",
    "        \n",
    "        # TODO: Implement the transition, observation, and noise covariance matrices.\n",
    "        self.transition_covariance = # TODO\n",
    "        self.observation_covariance = # TODO\n",
    "        self.transition_matrix = # TODO\n",
    "        self.observation_matrix = # TODO\n",
    "\n",
    "        # TODO: Implement the initial covariance and mean of the zero state.\n",
    "        self.mu_zero = # TODO\n",
    "        self.cov_zero = # TODO\n",
    "        \n",
    "    def filter(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Calculate the filtered mean and covariances of the latent space.\n",
    "\n",
    "        Args:\n",
    "            data: Observations with shape [n_observations, dim_x]\n",
    "\n",
    "        Returns:\n",
    "            Filtered mean and covariance for the latent space. The first dimension\n",
    "            of both should be n_observations+1 since the initial latent state has no\n",
    "            paired observation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Make sure the dimensions match and create some placeholders for the outputs.\n",
    "        n_observations, dim_x = data.shape\n",
    "        assert dim_x == self.dim_x\n",
    "        filtered_means = np.zeros((n_observations + 1, self.dim_z))\n",
    "        filtered_covs = np.zeros((n_observations + 1, self.dim_z, self.dim_z))\n",
    "        \n",
    "        # TODO: Implement filtering.\n",
    "            \n",
    "        return filtered_means, filtered_covs\n",
    "\n",
    "    def smooth(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Calculate the smoothed mean and covariances of the latent space.\n",
    "\n",
    "        Args:\n",
    "            data: Observations with shape [n_observations, dim_x]\n",
    "\n",
    "        Returns:\n",
    "            Smoothed mean and covariance for the latent space. The first dimension\n",
    "            of both should be n_observations+1 since the initial latent state has no\n",
    "            paired observation.        \n",
    "        \"\"\"\n",
    "        # Validate the data dimensions.\n",
    "        n_observations, dim_x = data.shape\n",
    "        assert dim_x == self.dim_x\n",
    "        \n",
    "        # Run the forward path\n",
    "        filtered_means, filtered_covs = self.filter(data)\n",
    "        \n",
    "        # Create holders for outputs\n",
    "        smoothed_means = np.zeros((n_observations + 1, self.dim_z))\n",
    "        smoothed_covs = np.zeros((n_observations + 1, self.dim_z, self.dim_z))\n",
    "        \n",
    "        # TODO: Implement smoothing.\n",
    "\n",
    "        return smoothed_means, smoothed_covs  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's quickly check our values match.\n",
    "kf_test = KalmanFilter(sigma_w=1.1, sigma_v=1.4, a=0.2, c=1.0, dim_z=2, dim_x=2, \n",
    "                       sigma_w_zero=1.8, mu_zero=np.array([0.2, 2.0]))\n",
    "\n",
    "filt_mean_test, filt_cov_test = kf_test.filter(np.array([[0.1, 0.2],[0.3,0.4]]))\n",
    "mean_expected = np.array([[0.2, 2.0], [0.064359, 0.318802],[0.124235, 0.194171]])\n",
    "cov_expected = np.array([[[3.24, 0.0], [0.0, 3.24]],\n",
    "                         [[0.79573767, 0.0],[0.0, 0.79573767]],\n",
    "                         [[0.76018596, 0.0],[0.0, 0.76018596]]])\n",
    "np.testing.assert_array_almost_equal(filt_mean_test, mean_expected) \n",
    "np.testing.assert_array_almost_equal(filt_cov_test, cov_expected) \n",
    "\n",
    "smooth_mean_test, smooth_cov_test = kf_test.smooth(np.array([[0.1, 0.2],[0.3,0.4]]))\n",
    "mean_expected = np.array([[0.218687, 1.968807], [0.078631, 0.335515], [0.124235, 0.194171]])\n",
    "cov_expected = np.array([[[3.11088996, 0.0],\n",
    "                          [0.0, 3.11088996]],\n",
    "                         [[0.78782721, 0.0],\n",
    "                          [0.0, 0.78782721]],\n",
    "                         [[0.76018596, 0.0],\n",
    "                          [0.0, 0.76018596]]])\n",
    "np.testing.assert_array_almost_equal(smooth_mean_test, mean_expected) \n",
    "np.testing.assert_array_almost_equal(smooth_cov_test, cov_expected) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load our data and see what the results of our filtering and smoothing operations are. Let's start with the simplest choice for the parameters of our model:\n",
    "$$ \\sigma_w = 1.0, \\sigma_v = 1.0, a=1.0, c=1.0, \\sigma_{w,0} = 1.0, \\mu_{0} = [0.0,0.0]\n",
    "$$\n",
    "\n",
    "You will have to\n",
    "* Conduct Kalman filtering to get the Kalman filtering means and covariances and plot their values.\n",
    "* Conduct Kalman smoothing to get the Kalman smoothing means and covariances and plot their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the data.\n",
    "data = # TODO\n",
    "x_data = # TODO\n",
    "y_data = # TODO\n",
    "time = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Conduct Kalman filtering\n",
    "data_stack = # TODO - check the args of the class you wrote.\n",
    "kalman_filter = # TODO\n",
    "filtered_mean, filtered_covariances = # TODO\n",
    "\n",
    "# Now let the visualization tools show you what you have.\n",
    "colors = ['#a1dab4','#41b6c4','#225ea8']\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "plot_means_and_cov(filtered_mean, filtered_covariances, ax, color=colors[0], label='Filtered Latent State')\n",
    "ax.plot(x_data, y_data, '.', color=colors[2], label='Observations')\n",
    "plot_true_path(time, ax)\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Conduct Kalman smoothing\n",
    "smooth_mean, smooth_covariances = # TODO\n",
    "\n",
    "# Now let the visualization tools show you what you have.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "plot_means_and_cov(smooth_mean, smooth_covariances, ax, color=colors[1], label='Smoothed Latent State')\n",
    "ax.plot(x_data, y_data, '.', color=colors[2], label='Observations')\n",
    "plot_true_path(time, ax)\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice any difference between the smoothing and filtering distributions? What is the intuition for the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Understanding our Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next week we will learn the parameters of our LDS from the data, but this week let's build some intution. Let's start with the noise on our observations $\\sigma_v$. Repeat the fitting / smoothing calculations like before, but with $\\sigma_v=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Conduct Kalman filtering\n",
    "kalman_filter = # TODO\n",
    "filtered_mean, filtered_covariances = # TODO\n",
    "\n",
    "# Now let the visualization tools show you what you have.\n",
    "colors = ['#a1dab4','#41b6c4','#225ea8']\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "plot_means_and_cov(filtered_mean, filtered_covariances, ax, color=colors[0], label='Filtered Latent State')\n",
    "ax.plot(x_data, y_data, '.', color=colors[2], label='Observations')\n",
    "plot_true_path(time, ax)\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Conduct Kalman smoothing\n",
    "smooth_mean, smooth_covariances = # TODO\n",
    "\n",
    "# Now let the visualization tools show you what you have.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "plot_means_and_cov(smooth_mean, smooth_covariances, ax, color=colors[1], label='Smoothed Latent State')\n",
    "ax.plot(x_data, y_data, '.', color=colors[2], label='Observations')\n",
    "plot_true_path(time, ax)\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the filtering and smoothing distributions behaving? Does this make sense given the parameters we changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, you're probably convinced that the noise in the observation isn't small. But what about the latent space? Let's try $\\sigma_w = 0.1$ and see how that changes our results.\n",
    "\n",
    "__Don't forget to change $\\sigma_v$ back__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Conduct Kalman filtering\n",
    "kalman_filter = # TODO\n",
    "filtered_mean, filtered_covariances = # TODO\n",
    "\n",
    "# Now let the visualization tools show you what you have.\n",
    "colors = ['#a1dab4','#41b6c4','#225ea8']\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "plot_means_and_cov(filtered_mean, filtered_covariances, ax, color=colors[0], label='Filtered Latent State')\n",
    "ax.plot(x_data, y_data, '.', color=colors[2], label='Observations')\n",
    "plot_true_path(time, ax)\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Conduct Kalman smoothing\n",
    "smooth_mean, smooth_covariances = # TODO\n",
    "\n",
    "# Now let the visualization tools show you what you have.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "plot_means_and_cov(smooth_mean, smooth_covariances, ax, color=colors[1], label='Smoothed Latent State')\n",
    "ax.plot(x_data, y_data, '.', color=colors[2], label='Observations')\n",
    "plot_true_path(time, ax)\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the smoothing and filtering distribution compare to the high $\\sigma_w$ case? What is the intuition behind this?\n",
    "\n",
    "What about the uncertanties of the filtering and smoothing distirbution? Which has larger covariances matrix values? What is the intuition behind this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final test, let's keep $\\sigma_w=0.3$ since that worked well, but start with a really bad initial guess for the latent space:\n",
    "$$\\mu_0 = [-2.0, 5.0].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Conduct Kalman filtering\n",
    "kalman_filter = # TODO\n",
    "filtered_mean, filtered_covariances = # TODO\n",
    "\n",
    "# Now let the visualization tools show you what you have.\n",
    "colors = ['#a1dab4','#41b6c4','#225ea8']\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "plot_means_and_cov(filtered_mean, filtered_covariances, ax, color=colors[0], label='Filtered Latent State')\n",
    "ax.plot(x_data, y_data, '.', color=colors[2], label='Observations')\n",
    "plot_true_path(time, ax)\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Conduct Kalman smoothing\n",
    "smooth_mean, smooth_covariances = # TODO\n",
    "\n",
    "# Now let the visualization tools show you what you have.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "plot_means_and_cov(smooth_mean, smooth_covariances, ax, color=colors[1], label='Smoothed Latent State')\n",
    "ax.plot(x_data, y_data, '.', color=colors[2], label='Observations')\n",
    "plot_true_path(time, ax)\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do our filtering and smoothing distribution recover from the bad initial guess? Which set of distributions is more sensitive to the intial guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "\n",
    "raise ValueError('Answer the question.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3f6ccb4d9c2992e26314c6a55f25a363977377544d6a8d30ed481869f911c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
