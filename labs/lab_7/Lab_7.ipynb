{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview__: This lab is meant as an introduction to particle filtering.\n",
    "\n",
    "__Goals__: Students should:\n",
    "\n",
    "1. Be able to implement the particle filtering algorithm to reconstruct the filtering distributions.\n",
    "2. Use the particle filtering algorithm to predict future latent states and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for our distribution p(observation|latent)\n",
    "GAUSS_WEIGHTS = [0.3, 0.3, 0.4]\n",
    "GAUSS_MEANS = [np.zeros(2), np.array([2.0,0.6]), np.array([-1.4,0.2])]\n",
    "GAUSS_COVS = [np.eye(2) * 3.0, np.eye(2)*2.0, np.eye(2) * 0.1]\n",
    "\n",
    "def observation_probability(latent: np.ndarray, observation: np.ndarray) -> float:\n",
    "    \"\"\"Given an observation and corresponding latent state, evaluate the likelihood.\n",
    "\n",
    "    Args:\n",
    "        latent: Latent state at current time step.\n",
    "        observation: Observation at current time step.\n",
    "\n",
    "    Returns:\n",
    "        Likelihood p(observation|latent).\n",
    "    \"\"\"\n",
    "    # Let's keep things somewhat 'simple' by making our distribution a sum of Gaussians\n",
    "    likelihood = 0\n",
    "    for weight, mean, cov in zip(GAUSS_WEIGHTS, GAUSS_MEANS, GAUSS_COVS):\n",
    "        evaluation_mean = latent + mean\n",
    "        likelihood += (\n",
    "            weight * stats.multivariate_normal(mean=evaluation_mean, cov=cov).pdf(observation)\n",
    "        )\n",
    "    return likelihood\n",
    "\n",
    "def observation_sample(latent: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Given the latent state, sample a observation.\n",
    "\n",
    "    Args:\n",
    "        latent: Latent state at current time step.\n",
    "\n",
    "    Returns:\n",
    "        Sampled observation.\n",
    "    \"\"\"\n",
    "    # Let's keep things somewhat 'simple' by making our distribution a sum of Gaussians\n",
    "    index = np.random.choice(np.arange(3), p=GAUSS_WEIGHTS)\n",
    "    evaluation_mean = latent + GAUSS_MEANS[index]\n",
    "    cov = GAUSS_COVS[index]\n",
    "    return stats.multivariate_normal(mean=evaluation_mean, cov=cov).rvs()\n",
    "\n",
    "def latent_sample(latent: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Given the latent state, sample the next latent state.\n",
    "\n",
    "    Args:\n",
    "        latent: Latent state at current time step.\n",
    "\n",
    "    Returns:\n",
    "        Sampled latent state.\n",
    "    \"\"\"\n",
    "    # Let's keep things somewhat 'simple' by making our distribution a sum of Gaussians\n",
    "    sample = np.zeros(len(latent))\n",
    "    sample[0] = np.sin(latent[1]) + np.random.randn() * 0.05\n",
    "    sample[1] = np.cos(latent[0]) + np.random.randn() * 0.05\n",
    "    sample += 1.0 * latent\n",
    "    return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Where's the rat again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend in the biology department is back. Apparently they wanted to build on last week and they kept playing music to their rat. They didn't get much of a response until they started playing _your favorite song_. That's right, _your favorite song_. What are the odds? Anyways, the rat started moving wildly and the sensors once again struggled to track the rat.\n",
    "\n",
    "Thankfully, since last week, your friend has been putting a lot work into building a physically motivated model for the rat's motion and the observations. They come to you with the data and three different distributions:\n",
    "\n",
    "$$\n",
    "z_t = g(z_{t-1})\n",
    "$$\n",
    "$$\n",
    "x_t = f(z_{t-1})\n",
    "$$\n",
    "$$\n",
    "p(x_t | z_t),\n",
    "$$\n",
    "where $g$ and $f$ are sampling functions for the latent states / observations, and $p(x_t | z_t)$ is the observation probability density function. Your friend was so on top of it this week, that they already loaded them into your notebook as _latent_sample_, _observation_sample_, and _observation_probability_.\n",
    "\n",
    "They hand you the data of the observed motion of the rat, and they ask you to reconstruct its latent state once again. \"It's going to be Gaussians isn't it,\" they say. Oh boy, are they in for a treat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we generate the data for you. You will then have to implement the particle filtering class. This will require:\n",
    "\n",
    "* Writing a function to compute the weights given latent space samples.\n",
    "* Writing a function to compute the particle filtering samples for all time steps.\n",
    "* Writing a function to predict future latent states and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a true set of latent states and observations for our analysis.\n",
    "latent_states = np.zeros((101,2))\n",
    "observation_states = np.zeros((len(latent_states)-1,2))\n",
    "t_observed = np.arange(len(observation_states))\n",
    "np.random.seed(3)\n",
    "for i in range(1, len(latent_states)):\n",
    "    latent_states[i] = latent_sample(latent_states[i-1])\n",
    "    observation_states[i-1] = observation_sample(latent_states[i])\n",
    "\n",
    "\n",
    "# Let's visualize our data.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "fontsize = 15\n",
    "plt.scatter(observation_states[:,0], observation_states[:,1], c=t_observed, \n",
    "            cmap='autumn', label='Observed Rat Positions')\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Time Index', fontsize=fontsize, rotation=270)\n",
    "plt.xlabel('X-Position', fontsize=fontsize)\n",
    "plt.ylabel('Y-Position', fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleFiltering:\n",
    "    \"\"\"Class that implements the Particle Filter.\n",
    "    \n",
    "    Args:\n",
    "        dim_z: Dimension of latent space.\n",
    "        dim_x: Dimension of observation space.\n",
    "        sigma_w_zero: Initial standard deviation of the zero state.\n",
    "        mu_zero: Initial mean of the zero state.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_z: int, dim_x: int, sigma_w_zero: float, \n",
    "                 mu_zero: np.ndarray):\n",
    "        \"\"\"Initialize our class.\"\"\"\n",
    "        # Save a few variables for bookkeeping\n",
    "        self.dim_x = dim_x\n",
    "        self.dim_z = dim_z\n",
    "\n",
    "        # Implement the initial covariance and mean of the zero state.\n",
    "        self.mu_zero = mu_zero\n",
    "        self.cov_zero = sigma_w_zero ** 2 * np.eye(dim_z)\n",
    "\n",
    "    def particle_filter(self, observations: np.ndarray, n_samples: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Run particle filtering algorithm to get samples for each z_t.\n",
    "\n",
    "        Args:\n",
    "            n_samples: Number of samples to draw of each state.\n",
    "\n",
    "        Returns:\n",
    "            Particle filtering samples and weights for each time step.\n",
    "        \"\"\"\n",
    "        # Placeholder for all of our samples and weights.\n",
    "        z_samples = np.zeros((len(observations) + 1, n_samples, self.dim_z))\n",
    "        weights = np.zeros((len(observations) + 1, n_samples))\n",
    "        \n",
    "        # Draw initial samples and set initial weights.\n",
    "        z_samples[0] = # TODO \n",
    "        weights[0] = # TODO \n",
    "        \n",
    "        # Now let's start our particle filtering loop.\n",
    "        for time in range(1, len(observations) + 1):\n",
    "\n",
    "            # Sample from the next latent state given the current latent state.\n",
    "            for samp_i in range(n_samples):\n",
    "                # Pick a sample with probability equal to its weight (resampling).\n",
    "                sample_choice = # TODO \n",
    "\n",
    "                # Move the selected sample and save it.\n",
    "                z_samples[time, samp_i] = # TODO \n",
    "\n",
    "            # Compute the weights for each of our new samples.\n",
    "            weights[time] = # TODO \n",
    "\n",
    "        return z_samples, weights\n",
    "\n",
    "    def compute_w(self, observation_t: np.ndarray, z_samples_t: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the importance weights for our sample of latent states.\n",
    "\n",
    "        Args:\n",
    "            observation_t: Current observation.\n",
    "            z_samples_t: Current latent state samples.\n",
    "\n",
    "        Returns:\n",
    "            Weights for each sample.\n",
    "        \"\"\"\n",
    "        # Placeholder for the weights.\n",
    "        weights_t = np.zeros(len(z_samples_t))\n",
    "\n",
    "        # Calculate each weight. Don't forget to normalize at the end!\n",
    "        for i in range(len(weights_t)):\n",
    "            weights_t[i] = # TODO \n",
    "        weights_t /= # TODO \n",
    "            \n",
    "        return weights_t\n",
    "\n",
    "    def predict(self, final_latent_samples: np.ndarray, final_weights: np.ndarray, \n",
    "                n_future: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Predict the distribution of future latent states and observations.\n",
    "\n",
    "        Args:\n",
    "            final_latent_samples: Latent samples for t=T.\n",
    "            final_weights: Weights for t=T.\n",
    "            n_future: Number of steps in the future to predict.\n",
    "\n",
    "        Returns:\n",
    "            Latent state samples and observations for the future.\n",
    "        \"\"\"\n",
    "        # Placeholder for our future samples, weights, and observations.\n",
    "        n_samples = len(final_latent_samples)\n",
    "        z_future = np.zeros((n_future, n_samples, self.dim_z))\n",
    "        obs_future = np.zeros((n_future, n_samples, self.dim_z))\n",
    "\n",
    "        # It will be useful to keep track of the current weights\n",
    "        # and samples.\n",
    "        z_current = final_latent_samples\n",
    "        weights_current = final_weights\n",
    "\n",
    "        # Iterate through our time steps:\n",
    "        for time_i in range(n_future):\n",
    "            \n",
    "            # Sample from the next latent state given the current latent state.\n",
    "            for samp_i in range(n_samples):\n",
    "                # Pick a sample with probability equal to its weight (resampling).\n",
    "                sample_choice = # TODO \n",
    "\n",
    "                # Move the selected sample and save it.\n",
    "                z_future[time_i, samp_i] = # TODO \n",
    "\n",
    "                # Generate an observation for the selected sample.\n",
    "                obs_future[time_i, samp_i] = # TODO \n",
    "\n",
    "            # After the first step, the weights are uniform since there is no more\n",
    "            # observations.\n",
    "            weights_current = # TODO \n",
    "            weights_current /= # TODO \n",
    "            z_current = # TODO \n",
    "\n",
    "        return z_future, obs_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_test = ParticleFiltering(dim_z = 2, dim_x = 2, sigma_w_zero = 0.5, mu_zero = np.array([0.2,0.4]))\n",
    "\n",
    "# Make sure the weight calculations are correct.\n",
    "test_observation = np.array([0.2, 0.4])\n",
    "test_latent_states = np.array([[-0.4, 1.2], [0.3, 1.2], [-2.2, 4.2]])\n",
    "np.testing.assert_array_almost_equal(\n",
    "    pf_test.compute_w(test_observation, test_latent_states),\n",
    "    np.array([0.53029462, 0.45243585, 0.01726953])\n",
    ")\n",
    "\n",
    "# Make sure the sampling calculations are correct.\n",
    "np.random.seed(5)\n",
    "test_observations = np.array([[-0.4, 1.2], [0.3, 1.2], [-2.2, 4.2]])\n",
    "n_test_samples = 100\n",
    "# Sadly, the remaining functions are very hard to test since they rely on the way you\n",
    "# called your random functions and your version of numpy. We offer some commented tests \n",
    "# that reproduce the behavior of the solutions.\n",
    "z_test_samples, weights_test = pf_test.particle_filter(test_observations, n_test_samples)\n",
    "# np.testing.assert_array_almost_equal(\n",
    "#     np.sum(z_test_samples * weights_test[:,:,np.newaxis], axis = 1),\n",
    "#     np.array([[0.193547, 0.465832],\n",
    "#               [0.785028, 1.162211],\n",
    "#               [1.889398, 1.370276],\n",
    "#               [2.630004, 1.39772 ]])\n",
    "# )\n",
    "# np.testing.assert_array_almost_equal(\n",
    "#     z_test_samples[:,0],\n",
    "#     np.array([[0.420614, 0.234565],\n",
    "#               [1.109561, 2.177135],\n",
    "#               [1.403333, 2.201349],\n",
    "#               [2.641904, 0.801542]])\n",
    "# )\n",
    "# np.testing.assert_array_almost_equal(\n",
    "#     weights_test[:,0],\n",
    "#     np.array([0.01, 0.001003, 0.001997, 0.00801])\n",
    "# )\n",
    "\n",
    "# Make sure the predictions are correct.\n",
    "np.random.seed(5)\n",
    "z_test_future, obs_test_future = pf_test.predict(\n",
    "    z_test_samples[-1], weights_test[-1], n_future=2\n",
    ")\n",
    "# Same caveat to these tests as before.\n",
    "# np.testing.assert_array_almost_equal(\n",
    "#     np.mean(z_test_future, axis = 1),\n",
    "#     np.array([[ 3.451687,  0.543841],\n",
    "#               [ 3.889172, -0.343928]])\n",
    "# )\n",
    "# np.testing.assert_array_almost_equal(\n",
    "#     np.mean(obs_test_future, axis = 1),\n",
    "#     np.array([[3.457603, 0.905334],\n",
    "#               [3.877964, 0.130703]])\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Running the Particle Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the particle filtering algorithm with sigma_w_zero = 1.1 and mu_zero = [1,1] and generate $N=100$ samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particle filter our rat position data.\n",
    "np.random.seed(3)\n",
    "pf_class = # TODO \n",
    "z_samples, weights = # TODO \n",
    "\n",
    "# Get the expected value of z at each timestep.\n",
    "z_mean = np.sum((z_samples * weights[:,:,np.newaxis]), axis=1)\n",
    "\n",
    "# Let's visualize our mean fit.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "fontsize = 15\n",
    "plt.scatter(observation_states[:,0], observation_states[:,1], c=t_observed, \n",
    "            cmap='autumn', label='Observed Rat Positions', alpha=0.2)\n",
    "\n",
    "plt.scatter(z_mean[1:,0], z_mean[1:,1], c=t_observed,\n",
    "            cmap='summer', label='Filtered Latent States', s=80)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Time Index', fontsize=fontsize, rotation=270)\n",
    "plt.xlabel('X-Position', fontsize=fontsize)\n",
    "plt.ylabel('Y-Position', fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our fit.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "fontsize = 15\n",
    "plt.scatter(z_mean[1:,0], z_mean[1:,1], c=t_observed,\n",
    "            cmap='summer', label='Filtered Latent States', alpha=0.8, s=80)\n",
    "plt.scatter(latent_states[1:,0], latent_states[1:,1], c=t_observed, marker='x',\n",
    "            cmap='summer', label='True Latent States', alpha=0.8, s=80)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Time Index', fontsize=fontsize, rotation=270)\n",
    "plt.xlabel('X-Position', fontsize=fontsize)\n",
    "plt.ylabel('Y-Position', fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Let's visualize our errors.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "fontsize = 15\n",
    "\n",
    "plt.plot(t_observed, np.abs(z_mean[1:,0] - latent_states[1:,0]), '.-', label='Error in X', c='#b2e2e2')\n",
    "plt.plot(t_observed, np.abs(z_mean[1:,1] - latent_states[1:,1]), '.-', label='Error in Y', c='#238b45')\n",
    "plt.xlabel('Time Index', fontsize=fontsize)\n",
    "plt.ylabel('Absolute Error', fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the error of our later latent states compare to the error of our earlier latent states? Why might this be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Predicting with the Particle Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the particle filtering algorithm with sigma_w_zero = 1.1 and mu_zero = [0,0] and generate $N=200$ samples. Then predict the future observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particle filter our rat position data.\n",
    "np.random.seed(3)\n",
    "pf_class = # TODO \n",
    "z_samples, weights = # TODO \n",
    "z_future, obs_future = # TODO \n",
    "\n",
    "# Get the range of expected values of the latent state and observations\n",
    "# at each timestep.\n",
    "z_future_mean = np.mean((z_future), axis=1)\n",
    "# We're visualizing our distribution as though it's Gaussian. \n",
    "z_future_std = np.std((z_future), axis=1)\n",
    "obs_future_mean = np.mean((obs_future), axis=1)\n",
    "obs_future_std = np.std((obs_future), axis=1)\n",
    "\n",
    "# Let's visualize if the true path falls within our range.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "fontsize = 15\n",
    "\n",
    "plt.plot(t_observed[80:], latent_states[81:,0], label='Latent State Truth', color='#d95f02', lw=6)\n",
    "plt.fill_between(t_observed[80:], z_future_mean[:,0] - 2 * z_future_std[:,0], \n",
    "                 z_future_mean[:,0] + 2 * z_future_std[:,0], \n",
    "                 label='Latent Distribution Prediction', alpha=0.4, color='#7570b3')\n",
    "plt.xlabel('Time Index', fontsize=fontsize)\n",
    "plt.ylabel('Latent X Position', fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "fontsize = 15\n",
    "\n",
    "plt.plot(t_observed[80:], observation_states[80:,0], label='Observation Truth', color='#1f78b4', lw=6)\n",
    "plt.fill_between(t_observed[80:], obs_future_mean[:,0] - 2 * obs_future_std[:,0], \n",
    "                 obs_future_mean[:,0] + 2 * obs_future_std[:,0], \n",
    "                 label='Observed Distribution Prediction', alpha=0.4, color='#b2df8a')\n",
    "plt.xlabel('Time Index', fontsize=fontsize)\n",
    "plt.ylabel('Observed X Position', fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
