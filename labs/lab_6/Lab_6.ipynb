{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dynamical Systems and Expectation-Maximization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview__: This lab is meant as an introduction to expectation-maximization and Kalman filtering.\n",
    "\n",
    "__Goals__: Students should:\n",
    "\n",
    "1. Be able to implement the expectation-maximization algorithm for latent space dynamical models.\n",
    "2. Be able to calculate the log posterior of an LDS model.\n",
    "3. Gain intuition for how iterations of the EM algorithm improve the quality of the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_means_and_cov(means: np.ndarray, covariances: np.ndarray, ax: Any, color: str, label: str):\n",
    "    \"\"\"Plot the mean and covariance of our filtering/smoothing.\n",
    "\n",
    "    Args:\n",
    "        means: Means to plot.\n",
    "        covariances: Covariances to plot.\n",
    "        ax: Axis on which to plot.\n",
    "        color: Color for plotting.\n",
    "        label: Label for plotted points.\n",
    "\n",
    "    Notes:\n",
    "        Will plot the 68% contours from the covariances.\n",
    "    \"\"\"\n",
    "    # Plot the trend line.\n",
    "    ax.plot(means[:,0], means[:,1], '-', color=color, label=label)\n",
    "\n",
    "    # Plot the ellipses for covariances. Assume they are diagonal (true for this lab).\n",
    "    for i in range(len(means)):\n",
    "        elip = Ellipse((means[i,0], means[i,1]), np.sqrt(covariances[i,0,0]), np.sqrt(covariances[i,1,1]), color=color, alpha=0.7)\n",
    "        ax.add_patch(elip)\n",
    "\n",
    "def sample_lds(n_timesteps: int, transition_matrix: np.ndarray, transition_covariance: np.ndarray, \n",
    "               observation_matrix: np.ndarray, observation_covariance: np.ndarray, mu_zero: np.ndarray, \n",
    "               cov_zero: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Sample from a latent dynamical system with the given parameters.\n",
    "\n",
    "    Args:\n",
    "        n_timesteps: Number of timesteps of data to sample.\n",
    "        transition_matrix: Transition matrix between latent states. This is A\n",
    "        transition_covariance: Covariance of latent state noise. This is Q\n",
    "        observation_matrix: Observation matrix from latent state to observation. This is C.\n",
    "        observation_covariance: Covariance of observation noise. This is R.\n",
    "        mu_zero: Mean of initial latent state.\n",
    "        cov_zero: Covariance of initial latent state.\n",
    "\n",
    "    Returns:\n",
    "        Latent and observed states from sampling.\n",
    "    \"\"\"\n",
    "    # In our model we assume there is no initial observation.\n",
    "    latent_state = np.zeros([n_timesteps+1, 2])\n",
    "    observed_state = np.zeros([n_timesteps, 2])\n",
    "\n",
    "    latent_state[0] = np.random.multivariate_normal(mu_zero, cov_zero)\n",
    "\n",
    "    # Iterate through latent states.\n",
    "    for t in range(n_timesteps):\n",
    "        latent_state[t+1] = (\n",
    "            np.dot(transition_matrix, latent_state[t]) + \n",
    "            np.random.multivariate_normal(np.zeros(2), transition_covariance)\n",
    "        )\n",
    "        observed_state[t] = (\n",
    "            np.dot(observation_matrix, latent_state[t+1]) + \n",
    "            np.random.multivariate_normal(np.zeros(2), observation_covariance)\n",
    "        )\n",
    "    return latent_state, observed_state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Expectation-Maximization for Latent Dynamical System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start this lab, we'll return to our same Kalman filtering / smoothing modeling from last week and fold in our new expectation-maximization algorithm. I have supplied you with a function for sampling a latent space and a set of observations that we will use to generate our data. You will have to import your solutions from last week into the class (with a small modification to the smoothing function). \n",
    "\n",
    "The expectation-maxiization equations were provided in this week's lecture, and there are test functions for each of the equations to make sure that your implementation is correct.\n",
    "\n",
    "* Import your solution for _init_, _filter_ and _smooth_ from last week's lab.\n",
    "* Fill out the _expectation_maximization_ function in the KalmanFilter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanFilter:\n",
    "    \"\"\"Class that implements the Kalman Filter for our LDS model.\n",
    "\n",
    "    Args:\n",
    "        sigma_w: Standard deviation of latent space noise.\n",
    "        sigma_v: Standard deviation of observation noise.\n",
    "        a: Magnitude of latent space transition matrix.\n",
    "        c: Magnitude of the observation matrix.\n",
    "        dim_z: Dimension of latent space.\n",
    "        dim_x: Dimension of observation space.\n",
    "        sigma_w_zero: Initial standard deviation of the zero state.\n",
    "        mu_zero: Initial mean of the zero state.\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma_w: float, sigma_v: float, a: float, c: float, dim_z: int, \n",
    "                 dim_x: int, sigma_w_zero: float, mu_zero: np.ndarray):\n",
    "        \"\"\"Initialize our class.\"\"\"\n",
    "        # Save a few variables for bookkeeping\n",
    "        self.dim_x = dim_x\n",
    "        self.dim_z = dim_z\n",
    "\n",
    "        # Used in the EM function.\n",
    "        self.smooth_matrices = None # The F_t matrices from the notes. Will need to be saved.\n",
    "        \n",
    "        # TODO: Implement the transition, observation, and noise covariance matrices.\n",
    "        self.transition_covariance = # TODO\n",
    "        self.observation_covariance = # TODO\n",
    "        self.transition_matrix = # TODO\n",
    "        self.observation_matrix = # TODO\n",
    "\n",
    "        # TODO: Implement the initial covariance and mean of the zero state.\n",
    "        self.mu_zero = # TODO\n",
    "        self.cov_zero = # TODO\n",
    "        \n",
    "    def filter(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Calculate the filtered mean and covariances of the latent space.\n",
    "\n",
    "        Args:\n",
    "            data: Observations with shape [n_observations, dim_x]\n",
    "\n",
    "        Returns:\n",
    "            Filtered mean and covariance for the latent space. The first dimension\n",
    "            of both should by n_observations+1 since the initial latent state has no\n",
    "            paired observation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Make sure the dimensions match and create some placeholders for the outputs.\n",
    "        n_observations, dim_x = data.shape\n",
    "        assert dim_x == self.dim_x\n",
    "        filtered_means = np.zeros((n_observations + 1, self.dim_z))\n",
    "        filtered_covs = np.zeros((n_observations + 1, self.dim_z, self.dim_z))\n",
    "        \n",
    "        # TODO: Implement filtering.\n",
    "            \n",
    "        return filtered_means, filtered_covs\n",
    "\n",
    "    def smooth(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Calculate the smoothed mean and covariances of the latent space.\n",
    "\n",
    "        Args:\n",
    "            data: Observations with shape [n_observations, dim_x]\n",
    "\n",
    "        Returns:\n",
    "            Smoothed mean and covariance for the latent space. The first dimension\n",
    "            of both should by n_observations+1 since the initial latent state has no\n",
    "            paired observation.\n",
    "\n",
    "        Notes:\n",
    "            \n",
    "        \"\"\"\n",
    "        # Validate the data dimensions.\n",
    "        n_observations, dim_x = data.shape\n",
    "        assert dim_x == self.dim_x\n",
    "        \n",
    "        # Run the forward path\n",
    "        filtered_means, filtered_covs = self.filter(data)\n",
    "        \n",
    "        # Create holders for outputs\n",
    "        smoothed_means = np.zeros((n_observations + 1, self.dim_z))\n",
    "        smoothed_covs = np.zeros((n_observations + 1, self.dim_z, self.dim_z))\n",
    "        self.smooth_matrices = np.zeros((n_observations + 1, self.dim_z, self.dim_z))\n",
    "        \n",
    "        # TODO: Implement smoothing.\n",
    "        \n",
    "        # Did you remember to save the F_t calculations for EM?\n",
    "        assert np.sum(np.abs(self.smooth_matrices)) > 0\n",
    "        # Did you remember to set the last F_t calculation? We don't need it for EM,\n",
    "        # but we do need it for completeness.\n",
    "        assert np.sum(np.abs(self.smooth_matrices[-1])) > 0\n",
    "\n",
    "        return smoothed_means, smoothed_covs  \n",
    "\n",
    "    def expectation_maximization(self, data: np.ndarray, n_iter: int):\n",
    "        \"\"\"Run the expectation-maximizaation algorithm on our LDS parameters.\n",
    "\n",
    "        Args:\n",
    "            data: Observations with shape [n_observations, dim_x].\n",
    "            n_iter: The number of iterations of the expectation-maximization algorithm to run.\n",
    "\n",
    "        Notes:\n",
    "            Updates the internal representations of the LDS parameters but has no outputs.\n",
    "        \"\"\"\n",
    "        # Validate the data dimensions.\n",
    "        n_observations, dim_x = data.shape\n",
    "        assert dim_x == self.dim_x\n",
    "        \n",
    "        # TODO: Implement EM Iterations.\n",
    "        for iter_num in range(n_iter):\n",
    "    \n",
    "            ### Expectation Step ###\n",
    "            # Get the smoothed state for the current model parameters.\n",
    "            smoothed_state_means, smoothed_state_covariances = # TODO\n",
    "            \n",
    "            # Solve for E[z_t], E[z_t z_{t-1}^T], E[z_t z_t^T] for use in EM.\n",
    "            expect_zt = # TODO\n",
    "            expect_zt_zt = # TODO\n",
    "            expect_zt_zt_minus = # TODO\n",
    "            \n",
    "            ### Maximization Step ###\n",
    "            # Update equation for initial state mean and covariance.\n",
    "            self.mu_zero = # TODO\n",
    "            self.cov_zero = # TODO\n",
    "            \n",
    "            # Update equation for transition matrix and noise covariance. \n",
    "            self.transition_matrix = # TODO\n",
    "            self.transition_covariance = # TODO\n",
    "            \n",
    "            # Update equation for observation matrix and noise covariance. \n",
    "            self.observation_matrix = # TODO\n",
    "            self.observation_covariance = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run some tests to make sure that your implementation of the expectation-maximization equations is correct.\n",
    "kf_test = KalmanFilter(sigma_w=0.9, sigma_v=0.98, a=0.9, c=1.2, dim_z=2, dim_x=2, sigma_w_zero=2.0, mu_zero=np.array([1.0, 1.2]))\n",
    "kf_test.expectation_maximization(np.array([[0.0, 0.05], [0.1, 0.2],[0.3,0.4]]), 1)\n",
    "\n",
    "# Start with the updated initial states\n",
    "np.testing.assert_almost_equal(kf_test.mu_zero, np.array([0.3185428, 0.4209496]))\n",
    "np.testing.assert_almost_equal(kf_test.cov_zero, np.array([[1.1333618, 0.0], [0.0, 1.1333618]]))\n",
    "np.testing.assert_almost_equal(kf_test.transition_matrix, np.array([[0.362484, 0.0247164], [0.0265544, 0.3868536]]))\n",
    "np.testing.assert_almost_equal(kf_test.transition_covariance, np.array([[0.3320965, 0.012569],[0.012569, 0.3414906]]))\n",
    "np.testing.assert_almost_equal(kf_test.observation_matrix, np.array([[0.0492384, 0.0705896], [0.076319, 0.1109947]]))\n",
    "np.testing.assert_almost_equal(kf_test.observation_covariance, np.array([[0.0298151, 0.0411604],[0.0411604, 0.0588818]]))\n",
    "\n",
    "# Make sure everything continues working for two iterations of EM\n",
    "kf_test = KalmanFilter(sigma_w=0.9, sigma_v=0.98, a=0.9, c=1.2, dim_z=2, dim_x=2, sigma_w_zero=2.0, mu_zero=np.array([1.0, 1.2]))\n",
    "kf_test.expectation_maximization(np.array([[0.0, 0.05], [0.1, 0.2],[0.3,0.4]]), 2)\n",
    "\n",
    "# Start with the updated initial states\n",
    "np.testing.assert_almost_equal(kf_test.mu_zero, np.array([0.4486251, 0.6279772]))\n",
    "np.testing.assert_almost_equal(kf_test.cov_zero, np.array([[1.1107989, -0.0337335], [-0.0337335, 1.0827763]]))\n",
    "np.testing.assert_almost_equal(kf_test.transition_matrix, np.array([[0.3938727, 0.0722462], [0.0729125, 0.4567033]]))\n",
    "np.testing.assert_almost_equal(kf_test.transition_covariance, np.array([[0.3326792, 0.013621], [0.013621, 0.3433377]]))\n",
    "np.testing.assert_almost_equal(kf_test.observation_matrix, np.array([[0.0538247, 0.078607], [0.0894437, 0.1314]]))\n",
    "np.testing.assert_almost_equal(kf_test.observation_covariance, np.array([[0.0278726, 0.0375556], [0.0375556, 0.0522982]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Where's the rat?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend in the biology department has been working for months to observe how different music affects a rat's behavior. They played Beethoven to their favorite rat for over an hour and they swear the rat was dancing a beautiful ballet. Unfortunately the optical data they captured tells a very different story: it just looks like a jumbled mess. They think the camera had some alignment issues. They turn to you, an expert in time-series analysis, to help them with their problem. __Can you reconstruct the rat's beautiful dance__?\n",
    "\n",
    "Let's take a look at the data your friend collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by generating the data our friend collected.\n",
    "n_timesteps = 100\n",
    "np.random.seed(1)\n",
    "\n",
    "transition_matrix = np.array([[1.01, -0.2],[0.2,0.95]])\n",
    "transition_covariance = 2.0*np.eye(2)\n",
    "\n",
    "observation_matrix = np.array([[1.0, 0.0],[0.2,1.0]])\n",
    "observation_covariance = 30.0*np.eye(2)\n",
    "\n",
    "mu_zero = np.array([1.0,1.2])\n",
    "cov_zero = 0.1*np.eye(2)\n",
    "\n",
    "latent_state, observed_state = sample_lds(n_timesteps, transition_matrix, transition_covariance, observation_matrix, observation_covariance, mu_zero, cov_zero)\n",
    "t_observed = np.arange(len(latent_state))\n",
    "\n",
    "# Let's plot what our friend collected.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "fontsize = 15\n",
    "plt.scatter(observed_state[:,0],observed_state[:,1], c=t_observed[1:], cmap='autumn', label='Observed Rat Positions')\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Time Index', fontsize=fontsize, rotation=270)\n",
    "plt.xlabel('X-Position', fontsize=fontsize)\n",
    "plt.ylabel('Y-Position', fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see where your friend is coming from, but it's really hard to discern an obvious pattern. You decide to model the problem as an LDS where the rat's dance is the latent state. \n",
    "\n",
    "* Apply the Kalman filter with the initial guess for the parameters.\n",
    "* Apply the Kalman filter to the data with 20 steps of expectation-maximization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll give you a smart initialization to your EM algorithm for free.\n",
    "kf = KalmanFilter(sigma_w=3.0, sigma_v=2.0, a=1.0, c=1.0, dim_z=2, dim_x=2, sigma_w_zero=1.0, mu_zero=np.array([1.0, 1.2]))\n",
    "\n",
    "# TODO: Get smooth latent states with the initial parameter choices. \n",
    "smooth_latent_means, smooth_latent_covariances = # TODO\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "fontsize = 15\n",
    "plt.scatter(observed_state[:,0],observed_state[:,1], c=t_observed[1:], cmap='autumn', label='Observed Rat Positions')\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Time Index', fontsize=fontsize, rotation=270)\n",
    "plt.xlabel('X-Position', fontsize=fontsize)\n",
    "plt.ylabel('Y-Position', fontsize=fontsize)\n",
    "plot_means_and_cov(smooth_latent_means, smooth_latent_covariances, ax=ax, color='#a1dab4', label='Estimated Latent Rat State')\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that doesn't inspire confidence. Let's try again, this time with 20 iteratiosn of EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll give you a smart initialization to your EM algorithm for free.\n",
    "kf = KalmanFilter(sigma_w=3.0, sigma_v=2.0, a=1.0, c=1.0, dim_z=2, dim_x=2, sigma_w_zero=1.0, mu_zero=np.array([1.0, 1.2]))\n",
    "\n",
    "# TODO: Run 20 iterations of expectation-maximization and extract the smooth latent states with the optimal parameters. \n",
    "smooth_latent_means, smooth_latent_covariances = # TODO (don't forget to do some EM first)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "fontsize = 15\n",
    "plt.scatter(observed_state[:,0],observed_state[:,1], c=t_observed[1:], cmap='autumn', label='Observed Rat Positions')\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Time Index', fontsize=fontsize, rotation=270)\n",
    "plt.xlabel('X-Position', fontsize=fontsize)\n",
    "plt.ylabel('Y-Position', fontsize=fontsize)\n",
    "plot_means_and_cov(smooth_latent_means, smooth_latent_covariances, ax=ax, color='#a1dab4', label='Estimated Latent Rat State')\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is! A beautiful dance. This rat was really feeling inspired by the song. For fun let's cheat and see how well we did of reconstructing the true latent states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "fontsize = 15\n",
    "plt.scatter(observed_state[:,0],observed_state[:,1], c=t_observed[1:], cmap='autumn', label='Observed Rat Positions')\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Time Index', fontsize=fontsize, rotation=270)\n",
    "plt.xlabel('X-Position', fontsize=fontsize)\n",
    "plt.ylabel('Y-Position', fontsize=fontsize)\n",
    "plot_means_and_cov(smooth_latent_means, smooth_latent_covariances, ax=ax, color='#a1dab4', label='Estimated Latent Rat State')\n",
    "plt.plot(latent_state[:,0], latent_state[:,1], '-', c='k', label='True Latent Rat State')\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, you should have done a good job of reconstructing the true latent states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3f6ccb4d9c2992e26314c6a55f25a363977377544d6a8d30ed481869f911c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
