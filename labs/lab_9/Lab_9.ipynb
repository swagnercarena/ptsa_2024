{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview__: This lab is meant as an introduction to Gaussian Process regression.\n",
    "\n",
    "__Goals__: Students should:\n",
    "\n",
    "1. Be able to implement the a Gaussian Process with a square exponential kernel.\n",
    "2. Gain intuition for how the kernel function parameters affect the prior.\n",
    "3. Gain intuition for how the kernel function parameters affect the prediction with and without noise.\n",
    "4. Be able to implement the data likelihood given the kernel parameters.\n",
    "5. Gain inuition for the tradeoff between fitting the data and model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinematics Dataset\n",
    "We will start from a pre-generated dataset for this lab. This data comes from a simple kinemtaics simulation that follows these equations:\n",
    "\n",
    "$$\n",
    "X_t = t * \\sin\\left( \\frac{2 \\pi t}{P} \\right) + W_t\n",
    "$$\n",
    "\n",
    "where $T$ is a parameter of our system and $W_t$ is white-noise with $W_t \\sim \\mathcal{N}(0, \\sigma_w^2$). This equation is the product of a periodic function and a linear function, so it will be interesting to see how different kernel assumptions for our Gaussian Process adapt to the data. Our goal is to infer the true trajectory of the object from the noisy data.\n",
    "\n",
    "We will generate a dataset where $P = 0.2$ and $\\sigma_w = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate the kinematics dataset.\n",
    "x = np.linspace(start=0, stop=10, num=1000).reshape(-1, 1)\n",
    "f_true = 0.5 * np.squeeze(x * np.sin(x / 0.2))\n",
    "\n",
    "rng = np.random.seed(2)\n",
    "obs_indices = np.random.choice(np.arange(f_true.size), size=10, replace=False)\n",
    "x_obs, f_obs = x[obs_indices], f_true[obs_indices]\n",
    "y_obs = f_obs + np.random.normal(loc=0.0, scale=0.5, size=f_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Implementing the Base Gaussian Process Class\n",
    "\n",
    "To compare different kernel you will need to implement a GP class that:\n",
    "\n",
    "1. Implement the calculation of the K matrix.\n",
    "2. Store the data and the inverse K matrix for prediction.\n",
    "3. Implement the mean and covariance prediction functions.\n",
    "4. Implement calculation of likelihood of observations given kernel parameters.\n",
    "\n",
    "We will assume that $\\mu(t) = 0$ for our GP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcess:\n",
    "    \"\"\"Class that implements a Gaussian Process.\n",
    "    \n",
    "    Args:\n",
    "        kernel_function: Function for calculating kappa(x,x').\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_function: Any):\n",
    "        \"\"\"Initialize our class.\"\"\"\n",
    "        # Save the vectorized kernel function. Your kernel function\n",
    "        # will take as input two scalar, but the vectorized function needs\n",
    "        # to take in two arrays, one of length $m$ and one of length $n$,\n",
    "        # and return a mxn matrix. We use one call of numpy.vectorize and take \n",
    "        # advantage of the signature options.\n",
    "        self._kernel_function = (\n",
    "            np.vectorize(kernel_function, \n",
    "                         signature='(),(m)->()'\n",
    "                        )\n",
    "        )\n",
    "\n",
    "        # Initialize to None so we know to predict the prior to start.\n",
    "        self.k_matrix_inv = None\n",
    "        self.observed_x = None\n",
    "        self.observed_y = None\n",
    "\n",
    "    def calc_k_matrix(self, x_rows: np.ndarray, x_cols: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the K matrix at the given positions.\n",
    "\n",
    "        Args:\n",
    "            x_rows: X-position for the rows.\n",
    "            x_cols: X-position for the columns.\n",
    "\n",
    "        Returns:\n",
    "            K(x_rows,x_cols) matrix.\n",
    "        \"\"\"\n",
    "        # TODO: All this function needs to do is call our vectorized kernel function\n",
    "        return # TODO\n",
    "        \n",
    "\n",
    "    def set_observations(self, observed_x: np.ndarray, observed_y: np.ndarray, \n",
    "                         sigma_w: float):\n",
    "        \"\"\"Store the x and y that have been observed.\n",
    "\n",
    "        Args:\n",
    "            observed_x: Observed input points x.\n",
    "            observed_y: Observed (noisy) outputs y.\n",
    "            sigma_w: Observational noise standard deviation.\n",
    "\n",
    "        Notes:\n",
    "            Modifies internal variables.\n",
    "        \"\"\"\n",
    "        # Save the observations.\n",
    "        self.observed_x = observed_x\n",
    "        self.observed_y = observed_y\n",
    "\n",
    "        # TODO: Save the K matrix inverse with the noise included.\n",
    "        self.k_matrix_inv = # TODO\n",
    "\n",
    "    def predict(self, predict_x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Predict the mean and covariance at given points.\n",
    "\n",
    "        Args:\n",
    "            predict_x: X-positions at which to make predictions.\n",
    "\n",
    "        Returns:\n",
    "            Mean and covariance matrix at the requested points.\n",
    "\n",
    "        Notes:\n",
    "            If no observations have been set, this will return the prior.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate the k_star_star matrix. \n",
    "        k_star_star_matrix = # TODO\n",
    "\n",
    "        # TODO: Start with the prior predictions\n",
    "        mean_pred = # TODO\n",
    "        cov_pred = # TODO\n",
    "\n",
    "        # TODO: Correct for observed data if it exists.\n",
    "        if self.observed_x is not None:\n",
    "            # Calculate the k_star matrix. Use the saved k_matrix_inv for speed \n",
    "            # of computation.\n",
    "            k_star_matrix = # TODO\n",
    "\n",
    "            # Update mean and covariance prediction.\n",
    "            mean_pred += # TODO\n",
    "            cov_pred -= # TODO\n",
    "        \n",
    "        return mean_pred, cov_pred\n",
    "\n",
    "    def sample_f(self, predict_x: np.ndarray, n_samples: Optional[int] = 1) -> np.ndarray:\n",
    "        \"\"\"Return samples of the function space at the desired points.\n",
    "\n",
    "        Args:\n",
    "            predict_x: X-positions at which to make predictions.\n",
    "            n_samples: Number of function samples to draw.\n",
    "\n",
    "        Returns:\n",
    "            Samples of the function space outputs.\n",
    "        \"\"\"\n",
    "        # Get the mean and covariance prediction from our function.\n",
    "        mean_pred, cov_pred = self.predict(predict_x)\n",
    "\n",
    "        # Draw samples.\n",
    "        f_samples = np.zeros((n_samples, len(predict_x)))\n",
    "        for i in range(len(f_samples)):\n",
    "            f_samples[i] = multivariate_normal.rvs(mean=mean_pred, cov=cov_pred)\n",
    "\n",
    "        return f_samples\n",
    "\n",
    "    def log_likelihood(self) -> float:\n",
    "        \"\"\"Return the log likelihood of the data for the given kernel.\n",
    "\n",
    "        Returns:\n",
    "            Log likelihood of the data.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the log likelihood of the data.\n",
    "        log_likelihood_calc = # TODO\n",
    "\n",
    "        return log_likelihood_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few tests for our GP functions.\n",
    "def test_kernel_function(x, x_prime):\n",
    "    return np.exp(-np.sum(np.square(x - x_prime))/20)\n",
    "\n",
    "gp_test = GaussianProcess(test_kernel_function)\n",
    "\n",
    "x_test = np.linspace(0,10,4).reshape(-1, 1)\n",
    "\n",
    "# Test the K matrix calculation.\n",
    "k_matrix = gp_test.calc_k_matrix(x_test, x_test)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    k_matrix,\n",
    "    [[1.0, 0.573753, 0.108368, 0.006738],\n",
    "     [0.573753, 1.0, 0.573753, 0.108368],\n",
    "     [0.108368, 0.573753, 1.0, 0.573753],\n",
    "     [0.006738, 0.108368, 0.573753, 1.0]]\n",
    ")\n",
    "\n",
    "# Test the predictions without observations.\n",
    "mean_test, cov_test = gp_test.predict(x_test)\n",
    "np.testing.assert_array_almost_equal(mean_test, np.zeros(4))\n",
    "np.testing.assert_array_almost_equal(\n",
    "    cov_test, \n",
    "    [[1.0, 0.573753, 0.108368, 0.006738],\n",
    "     [0.573753, 1.0, 0.573753, 0.108368],\n",
    "     [0.108368, 0.573753, 1.0, 0.573753],\n",
    "     [0.006738, 0.108368, 0.573753, 1.0]]\n",
    ")\n",
    "\n",
    "# Test the predictions with observations\n",
    "x_obs_test = np.linspace(0,10,4).reshape(-1, 1)\n",
    "y_obs_test = np.linspace(5,50,4)\n",
    "gp_test.set_observations(x_obs_test, y_obs_test, sigma_w=0.3)\n",
    "mean_test, cov_test = gp_test.predict(x_test)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    mean_test,\n",
    "    np.array([5.346965, 18.628744, 34.861521, 46.078638])\n",
    ")\n",
    "np.testing.assert_array_almost_equal(\n",
    "    cov_test, \n",
    "    [[ 0.078941,  0.007533, -0.003469,  0.001145],\n",
    "     [ 0.007533,  0.073929,  0.009536, -0.003469],\n",
    "     [-0.003469,  0.009536,  0.073929,  0.007533],\n",
    "     [ 0.001145, -0.003469,  0.007533,  0.078941]]\n",
    ")\n",
    "\n",
    "# Test the log likelihood calculation.\n",
    "np.testing.assert_almost_equal(\n",
    "    gp_test.log_likelihood(),\n",
    "    -1262.211359395657\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Comparing Kernels for our GP.\n",
    "\n",
    "Let's compare how our different kernels impact our predictions. We can also test how different kernel values are favored by the model. We will:\n",
    "\n",
    "1. Compare the prediction of a squared exponential and a cosine kernel.\n",
    "2. See how different choices of the period for our cosine kernel impact the likelihood of the observed data.\n",
    "\n",
    "The squared exponential kernel is given by:\n",
    "\n",
    "$$\n",
    "\\kappa(x,x') = A * \\exp\\left(\\frac{-(x-x')^2}{2 l^2}) \\right)\n",
    "$$\n",
    "\n",
    "The cosine kernel is given by:\n",
    "\n",
    "$$\n",
    "\\kappa(x,x') = A * \\cos\\left(\\frac{|x-x'|}{P}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to implement our two kernels.\n",
    "def cosine_kernel(x: float, x_prime: float, period: float, amplitude: float) -> float:\n",
    "    \"\"\"Cosine kernel function.\n",
    "\n",
    "    Args:\n",
    "        x: First x-position at which to calculate the kernel.\n",
    "        x_prime: Second x-position at which to calculate the kernel.\n",
    "        period: Period of the cosine kernel.\n",
    "        amplitude: Amplitude of the cosine kernel.\n",
    "\n",
    "    Returns:\n",
    "        Kernel function value.\n",
    "    \"\"\"\n",
    "    # TODO: Implement cosine kernel function.\n",
    "    return # TODO\n",
    "\n",
    "def squared_exp_kernel(x: float, x_prime: float, length_scale: float, amplitude: float) -> float:\n",
    "    \"\"\"Cosine kernel function.\n",
    "\n",
    "    Args:\n",
    "        x: First x-position at which to calculate the kernel.\n",
    "        x_prime: Second x-position at which to calculate the kernel.\n",
    "        length_scale: Length scale of the squared exponential kernel.\n",
    "        amplitude: Amplitude of the squared exponential kernel.\n",
    "\n",
    "    Returns:\n",
    "        Kernel function value.\n",
    "    \"\"\"\n",
    "    # TODO: Implement cosine kernel function.\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's visualize the squared exponential kernel.\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# TODO: Use functools.partial to set our kernel.\n",
    "length_scale = 1.0\n",
    "amplitude = 2.0\n",
    "kernel_function = # TODO\n",
    "gp = GaussianProcess(kernel_function)\n",
    "sigma_w = 0.5\n",
    "\n",
    "gp.set_observations(x_obs, y_obs, sigma_w)\n",
    "y_pred_mean, y_pred_cov = gp.predict(x)\n",
    "y_pred_std = np.sqrt(np.diag(y_pred_cov))\n",
    "plt.plot(x, y_pred_mean, color = '#99d8c9', label = 'GP Posterior Distribution')\n",
    "plt.plot(x_obs, y_obs, 'x', color = 'k', label = 'Observed Data', markersize=7)\n",
    "plt.plot(x, f_true, '--', color = 'k', label = 'True Generative Function')\n",
    "plt.fill_between(x[:,0], y_pred_mean - 2 * y_pred_std, y_pred_mean + 2 * y_pred_std, \n",
    "                 color = '#99d8c9', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('f(Time)')\n",
    "plt.ylim([-10,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Why are these predictions doing poorly? Limit your answer to the form of the kernel and not the specific parameters values.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next let's visualize the cosine kernel.\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# TODO: Use functools.partial to set our kernel.\n",
    "period = 0.2\n",
    "amplitude = 2.0\n",
    "kernel_function = # TODO\n",
    "gp = GaussianProcess(kernel_function)\n",
    "sigma_w = 0.5\n",
    "\n",
    "gp.set_observations(x_obs, y_obs, sigma_w)\n",
    "y_pred_mean, y_pred_cov = gp.predict(x)\n",
    "y_pred_std = np.sqrt(np.diag(y_pred_cov))\n",
    "plt.plot(x, y_pred_mean, color = '#99d8c9', label = 'GP Posterior Distribution')\n",
    "plt.plot(x_obs, y_obs, 'x', color = 'k', label = 'Observed Data', markersize=7)\n",
    "plt.plot(x, f_true, '--', color = 'k', label = 'True Generative Function')\n",
    "plt.fill_between(x[:,0], y_pred_mean - 2 * y_pred_std, y_pred_mean + 2 * y_pred_std, \n",
    "                 color = '#99d8c9', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('f(Time)')\n",
    "plt.ylim([-10,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How are these new predictions better than the previous? Why are the predictions still limited? Limit your answer to the form of the kernel and not the specific parameters values.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we combine our two kernels together?\n",
    "def combined_kernel(x: float, x_prime: float, period_cosine: float, amplitude_cosine: float, \n",
    "                    length_scale: float, amplitude_se: float) -> float:\n",
    "    \"\"\"Sum of cosine and squared exponential kernel.\n",
    "\n",
    "    Args:\n",
    "        x: First x-position at which to calculate the kernel.\n",
    "        x_prime: Second x-position at which to calculate the kernel.\n",
    "        period_cosine: Period of the cosine kernel.\n",
    "        amplitude_cosine: Amplitude of the cosine kernel.\n",
    "        length_scale: Length scale of the squared exponential kernel.\n",
    "        amplitude_se: Amplitude of the squared exponential kernel.\n",
    "\n",
    "    Returns:\n",
    "        Kernel function value.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the combination.\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's visualize the combined kernel.\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# TODO: Use functools.partial to set our kernel.\n",
    "period_cosine = 0.2\n",
    "amplitude_cosine = 2.0\n",
    "length_scale = 0.2\n",
    "amplitude_se = 2.0\n",
    "kernel_function = # TODO\n",
    "gp = GaussianProcess(kernel_function)\n",
    "sigma_w = 0.5\n",
    "\n",
    "gp.set_observations(x_obs, y_obs, sigma_w)\n",
    "y_pred_mean, y_pred_cov = gp.predict(x)\n",
    "y_pred_std = np.sqrt(np.diag(y_pred_cov))\n",
    "plt.plot(x, y_pred_mean, color = '#99d8c9', label = 'GP Posterior Distribution')\n",
    "plt.plot(x_obs, y_obs, 'x', color = 'k', label = 'Observed Data', markersize=7)\n",
    "plt.plot(x, f_true, '--', color = 'k', label = 'True Generative Function')\n",
    "plt.fill_between(x[:,0], y_pred_mean - 2 * y_pred_std, y_pred_mean + 2 * y_pred_std, \n",
    "                 color = '#99d8c9', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('f(Time)')\n",
    "plt.ylim([-10,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How are these new predictions better than the previous? Why might this be the case? Limit your answer to the form of the kernel and not the specific parameters values.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will compare the quality of different period choices.\n",
    "periods = np.linspace(0.1,0.3,1000)\n",
    "log_likelihoods = np.zeros(periods.shape)\n",
    "\n",
    "# Let's start by plotting few length scales.\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5), dpi=100)\n",
    "\n",
    "for i, period in enumerate([0.1, 0.2, 0.3]):\n",
    "    # TODO: Use the looped period parameter to initialize the kernel function.\n",
    "    amplitude_cosine = 2.0\n",
    "    length_scale = 0.2\n",
    "    amplitude_se = 2.0\n",
    "    kernel_function = # TODO\n",
    "    gp = GaussianProcess(kernel_function)\n",
    "    gp.set_observations(x_obs, y_obs, sigma_w)\n",
    "    y_pred_mean, y_pred_cov = gp.predict(x)\n",
    "    y_pred_std = np.sqrt(np.diag(y_pred_cov))\n",
    "    ax[i].plot(x, y_pred_mean, color = '#99d8c9', label = 'GP Posterior Distribution')\n",
    "    ax[i].plot(x_obs, y_obs, 'x', color = 'k', label = 'Observed Data', markersize=7)\n",
    "    ax[i].plot(x, f_true, '--', color = 'k', label = 'True Generative Function')\n",
    "    ax[i].fill_between(x[:,0], y_pred_mean - 2 * y_pred_std, y_pred_mean + 2 * y_pred_std, \n",
    "                       color = '#99d8c9', alpha=0.5)\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel('Time')\n",
    "    ax[i].set_ylabel('f(Time)')\n",
    "    ax[i].set_ylim([-10,10])\n",
    "    ax[i].set_title(f'Period = {period}')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# For each length scale, calculate the log likelihood.\n",
    "for i, period in enumerate(periods):\n",
    "    # TODO: Use the looped period parameter to initialize the kernel function.\n",
    "    amplitude_cosine = 2.0\n",
    "    length_scale = 0.2\n",
    "    amplitude_se = 2.0\n",
    "    kernel_function = # TODO\n",
    "    gp = GaussianProcess(kernel_function)\n",
    "    gp.set_observations(x_obs, y_obs, sigma_w)\n",
    "    log_likelihoods[i] = gp.log_likelihood()\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(periods, log_likelihoods)\n",
    "# plt.axvline(0.2, c='k')\n",
    "plt.xlabel(r'Period $P$', fontsize=20)\n",
    "plt.ylabel(r'$\\log p(\\mathbf{y}|\\mathbf{X},P)$', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What value of the period parameter maximizes the data likelihood? Does this make sense?__\n",
    "\n",
    "__How would you describe the predictions from $p=0.1$? Too complex or to too simple? Does this make sense?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
