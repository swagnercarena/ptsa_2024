{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview__: This lab is meant as an introduction to hidden Markov Models.\n",
    "\n",
    "__Goals__: Students should:\n",
    "\n",
    "1. Be able to implement the forward and backward pass for the discrete hidden markov model.\n",
    "2. Be able to implement the viterbi algorithm for discrete hidden markov models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for our distribution p(observation|latent). These will be long and\n",
    "# randomized\n",
    "np.random.seed(2)\n",
    "obs_dim = 3\n",
    "GAUSS_MEANS = [np.array([1.0, -1.0, 0.2]),\n",
    "               np.array([0.6, 1.1, 1.0]),\n",
    "               np.array([1.3, 1.0, 1.2])]\n",
    "GAUSS_COVS = [np.eye(obs_dim) / 4]*3\n",
    "PI = np.array([0.7, 0.2, 0.1])\n",
    "A_MAT = np.array([[0.9, 0.0, 0.1], [0.0, 0.9, 0.1], [0.1, 0.3, 0.6]])\n",
    "\n",
    "def generate_latent_and_observations(n_observtions: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate a sample set of observations along with the true latents.\n",
    "\n",
    "    Args:\n",
    "        n_observations: Number of observations to generate.\n",
    "\n",
    "    Returns:\n",
    "        Sequence of latent states and observations.\n",
    "    \"\"\"\n",
    "    # Placeholders.\n",
    "    latents = np.zeros((n_observtions + 1, len(PI)))\n",
    "    observations = np.zeros((n_observtions, len(GAUSS_MEANS[0])))\n",
    "\n",
    "    # Start with the initial latent state\n",
    "    latents[0, np.random.choice(np.arange(len(PI)), p=PI)] = 1\n",
    "\n",
    "    # Fill out the rest.\n",
    "    for i in range(n_observtions):\n",
    "        latents[i+1, np.random.choice(np.arange(len(PI)), p=A_MAT[np.argmax(latents[i])])] = 1\n",
    "        cur_state = np.argmax(latents[i+1])\n",
    "        observations[i] = np.random.multivariate_normal(GAUSS_MEANS[cur_state], GAUSS_COVS[cur_state])\n",
    "\n",
    "    return latents, observations\n",
    "\n",
    "def observation_probability(latent_index: int, observation: np.ndarray) -> float:\n",
    "    \"\"\"Given an observation and corresponding latent state, evaluate the likelihood.\n",
    "\n",
    "    Args:\n",
    "        latent_index: Index of latent state encoding.\n",
    "        observation: Observation at current time step.\n",
    "\n",
    "    Returns:\n",
    "        Likelihood p(observation|latent).\n",
    "    \"\"\"\n",
    "    # Let's keep things somewhat 'simple' by making our distribution a sum of Gaussians\n",
    "    return stats.multivariate_normal.pdf(observation, mean = GAUSS_MEANS[latent_index], cov = GAUSS_COVS[latent_index])\n",
    "\n",
    "# Generate the data for our study.\n",
    "true_latents, observations = generate_latent_and_observations(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Study\n",
    "\n",
    "Your friend in the sociology department is studying the effects of social media habits on emotions. They've run an experiment placing FMRI sensors on the brains of several volunteers as they scroll through the internet. The volunteers can be in one of three states: happy, angry, or neutral. They've used the following one-hot encoding:\n",
    "\n",
    "$$\n",
    "\\pmb{z}_\\text{happy} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\ \n",
    "\\pmb{z}_\\text{angry} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\\n",
    "\\pmb{z}_\\text{neutral} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Given that encoding, they also have a very good model for the probability of observing a specific FMRI scan $\\pmb{x}_t$ given the state $\\pmb{z}_t$. They have already done the favor of uploading it to your notebook as the function _observation_probability_. They also have a time series $\\{\\pmb{x}_1, \\ldots \\pmb{x}_T\\}$ for a volunteer that spends a lot of time on social media. For this volunteer, they know that the transition matrix looks like:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 0.9 & 0.0 & 0.1 \\\\ 0.0 & 0.9 & 0.1 \\\\ 0.1 & 0.3 & 0.6\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and having observed the volunteer enter they have a pretty good guess at the initial state:\n",
    "\n",
    "$$\n",
    "\\pi = \\begin{bmatrix} 0.7 \\\\ 0.2 \\\\ 0.1 \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "They would like to know:\n",
    "\n",
    "- __What is the most likely emotional state at each time step (i.e. max of  $p(z_t|x_{1:t})$ or $p(z_t|x_{1:T})$)?__\n",
    "- __What is the most likely sequence of emotional states expressed by the volunteer?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Implementing the Viterbi Algorithm for a discrete Hidden Markov Model.\n",
    "\n",
    "To answer your friend's questions, you will need to solve for the transition matrix governing the discrete HMM and calculate the most likely series of latent states. This will require:\n",
    "\n",
    "1. Implementing the calculation for $\\hat{\\alpha}(\\pmb{z}_t)$\n",
    "2. Implementing the calculation for $\\hat{\\beta}(\\pmb{z}_t)$\n",
    "3. Implementing the calculation for $p(\\pmb{z}_t | \\pmb{x}_{1:t}), \\ p(\\pmb{z}_t | \\pmb{x}_{1:T}), \\ p(\\pmb{z}_{1:T} | \\pmb{x}_{1:T})$\n",
    "4. Implementing the Viterbi algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A useful helper function.\n",
    "def one_hot_encode(state_index: int, dim_z: int) -> np.ndarray:\n",
    "    \"\"\"Return the one hot encoding for a latent state.\n",
    "\n",
    "    Args:\n",
    "        state_index: Latent state index. Counts from zero.\n",
    "        dim_z: Number of latent states.\n",
    "\n",
    "    Returns:\n",
    "        One-hot encoding.\n",
    "    \"\"\"\n",
    "    z_encode = np.zeros(dim_z)\n",
    "    z_encode[state_index] = 1\n",
    "    return z_encode\n",
    "\n",
    "\n",
    "# Our class of interest.\n",
    "class DiscreteHMM:\n",
    "    \"\"\"Class that implements a discrete HMM.\n",
    "    \n",
    "    Args:\n",
    "        pi: Initial guess for the vector pi.\n",
    "        transition_matrix: Initial guess for the transition matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, pi: np.ndarray, transition_matrix: np.ndarray):\n",
    "        \"\"\"Initialize our class.\"\"\"\n",
    "        # Save the initial guess.\n",
    "        self.pi = pi\n",
    "        self.transition_matrix = transition_matrix\n",
    "\n",
    "        # Some useful variables.\n",
    "        self.dim_z = len(self.pi)\n",
    "\n",
    "    def calc_alpha_hat(self, observations: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Given the observations, calculate the normalized forward pass.\n",
    "\n",
    "        Args:\n",
    "            observations: Observations for each time step.\n",
    "\n",
    "        Returns:\n",
    "            Normalized forward pass for each time step and latent state and \n",
    "            normalization constants.\n",
    "        \"\"\"\n",
    "        # Initialize a placeholder.\n",
    "        alpha_hat = np.zeros((len(observations) + 1, self.dim_z))\n",
    "        c_t = np.zeros((len(observations) + 1))\n",
    "\n",
    "        # t=0 value. Consider c_0 = 1 for simplicity.\n",
    "        alpha_hat[0] = # TODO\n",
    "        c_t[0] = 1\n",
    "\n",
    "        # Recurse forward.\n",
    "        for t in range(1, len(alpha_hat)):\n",
    "            for i in range(self.dim_z):\n",
    "                for j in range(self.dim_z):\n",
    "                    alpha_hat[t,i] += # TODO\n",
    "            # Don't forget to normalize alpha_hat!\n",
    "            c_t[t] = # TODO\n",
    "            alpha_hat[t] /= c_t[t]\n",
    "            \n",
    "        return alpha_hat, c_t\n",
    "\n",
    "    def calc_beta_hat(self, observations: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Given the observations, calculate the normalized backward pass.\n",
    "\n",
    "        Args:\n",
    "            observations: Observations for each time step.\n",
    "\n",
    "        Returns:\n",
    "            Normalized backward pass for each time step.\n",
    "        \"\"\"\n",
    "        # Initialize a placeholder.\n",
    "        beta_hat = np.zeros((len(observations) + 1, self.dim_z))\n",
    "\n",
    "        # Get the c_t values\n",
    "        _, c_t = self.calc_alpha_hat(observations)\n",
    "\n",
    "        # t=0 value.\n",
    "        beta_hat[-1] = # TODO\n",
    "\n",
    "        # Recurse backward.\n",
    "        for t in range(len(beta_hat)-2, -1, -1):\n",
    "            for i in range(self.dim_z):\n",
    "                for j in range(self.dim_z):\n",
    "                    beta_hat[t,i] += # TODO\n",
    "            # Don't forget to normalize beta_hat!\n",
    "            beta_hat[t] /= # TODO\n",
    "\n",
    "        return beta_hat\n",
    "\n",
    "    def p_zt_xt(self, observations: np.ndarray, alpha_hat: np.ndarray, \n",
    "                beta_hat: np.ndarray, c_t: np.ndarray):\n",
    "        \"\"\"Calculate p(z_t|x_{1:t}) for all t.\n",
    "\n",
    "        Args:\n",
    "            observations: Observations for each time step.\n",
    "            alpha_hat: Normalized forward pass output.\n",
    "            beta_hat: Normalized backward pass output.\n",
    "            c_t: Normalization constants for forward and backward pass.\n",
    "\n",
    "        Returns:\n",
    "            Value of p(z_t|x_{1:t}) for each time step and latent state index.\n",
    "\n",
    "        Notes:\n",
    "            You may not need all of the inputs.\n",
    "        \"\"\"\n",
    "        return # TODO\n",
    "    \n",
    "    def p_zt_xT(self, observations: np.ndarray, alpha_hat: np.ndarray, \n",
    "                beta_hat: np.ndarray, c_t: np.ndarray):\n",
    "        \"\"\"Calculate p(z_t|x_{1:T}) for all t.\n",
    "\n",
    "        Args:\n",
    "            observations: Observations for each time step.\n",
    "            alpha_hat: Normalized forward pass output.\n",
    "            beta_hat: Normalized backward pass output.\n",
    "            c_t: Normalization constants for forward and backward pass.\n",
    "\n",
    "        Returns:\n",
    "            Value of p(z_t|x_{1:T}) for each time step and latent state index.\n",
    "\n",
    "        Notes:\n",
    "            You may not need all of the inputs.\n",
    "        \"\"\"\n",
    "        return # TODO\n",
    "\n",
    "    def log_p_sequence_xt(self, observations: np.ndarray, latent_sequence: np.ndarray) -> float:\n",
    "        \"\"\"Log likelihood of the sequence given the data p(z_{1:T}|x_{1:T}).\n",
    "\n",
    "        Args:\n",
    "            observations: Observations for each time step.\n",
    "            latent_sequence: Proposed latent sequence. Can be a probability\n",
    "                distribution at each time step, in which case argmax will be taken\n",
    "                to determine proposal.\n",
    "\n",
    "        Returns:\n",
    "            Log of the probability of the sequence given the data.\n",
    "        \"\"\"\n",
    "        log_probability = 0\n",
    "        sequence_indices = np.argmax(latent_sequence, axis=-1)\n",
    "\n",
    "        # Log probability for first time step.\n",
    "        log_probability += # TODO\n",
    "\n",
    "        # Log probability for remaining time steps.\n",
    "        for t in range(len(observations)):\n",
    "            log_probability += # TODO\n",
    "\n",
    "        return log_probability\n",
    "        \n",
    "\n",
    "    def viterbi_algorithm(self, observations: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Run the Viterbi algorithm on our dHMM observations.\n",
    "\n",
    "        Args:\n",
    "            observations: Observations for each time step.\n",
    "\n",
    "        Returns:\n",
    "            One-hot encoding of MAP sequence.\n",
    "        \"\"\"\n",
    "        w_matrix = np.zeros((len(observations)+1, self.dim_z))\n",
    "        m_matrix = np.zeros((len(observations), self.dim_z), dtype=int)\n",
    "\n",
    "        # The first value of our weight matrix is just log of pi.\n",
    "        w_matrix[0] = # TODO\n",
    "\n",
    "        # Fill out the weight matrix and the argmax matrix m.\n",
    "        for t in range(len(observations)):\n",
    "            for i in range(self.dim_z):\n",
    "                # Let's find the best path to this state\n",
    "                m_matrix[t,i] = # TODO\n",
    "                w_matrix[t+1,i] = # TODO\n",
    "    \n",
    "        # Now we can work backwards to find the most likely path.\n",
    "        map_z_path = np.ones(len(observations)+1, dtype=int) - 1 # Make sure you fill it all in!\n",
    "\n",
    "        # MAP z_T.\n",
    "        map_z_path[-1] = # TODO\n",
    "\n",
    "        # Remaining z_t.\n",
    "        for t in range(len(observations)-1, -1, -1):\n",
    "            map_z_path[t] = # TODO\n",
    "\n",
    "        # Turn it into our one-hot encoding\n",
    "        map_z_encoded = np.zeros((len(observations) + 1, self.dim_z))\n",
    "        for t in range(len(map_z_path)):\n",
    "            map_z_encoded[t] = one_hot_encode(map_z_path[t], self.dim_z)\n",
    "\n",
    "        return map_z_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test all our functions.\n",
    "dhmm_test = DiscreteHMM(pi = PI, transition_matrix = A_MAT)\n",
    "test_observations = np.array(\n",
    "    [[ 0.57912632, -1.75050344,  0.6467949 ],\n",
    "     [ 1.27572702,  0.14610401,  0.2207697 ],\n",
    "     [ 0.44103728, -0.75995758,  0.39349526]]\n",
    ")\n",
    "\n",
    "# Test our alpha_hat calculation.\n",
    "aht, ctt = dhmm_test.calc_alpha_hat(test_observations)\n",
    "np.testing.assert_almost_equal(\n",
    "    aht,\n",
    "    [[7.0000000e-01, 2.0000000e-01, 1.0000000e-01],\n",
    "     [9.9999977e-01, 1.4661610e-07, 7.9086042e-08],\n",
    "     [9.4237462e-01, 5.0723353e-08, 5.7625330e-02],\n",
    "     [9.9993612e-01, 2.0604439e-05, 4.3274226e-05]]\n",
    ")\n",
    "np.testing.assert_almost_equal(\n",
    "    ctt,\n",
    "    [1.0, 0.0496036, 0.0300947, 0.1920015]\n",
    ")\n",
    "\n",
    "# Test the beta hat calculation.\n",
    "bht = dhmm_test.calc_beta_hat(test_observations)\n",
    "np.testing.assert_almost_equal(\n",
    "    bht,\n",
    "    [[1.4062500e+00, 1.2433363e-08, 1.5625005e-01],\n",
    "     [1.0000002e+00, 7.1046726e-03, 1.5114756e-01],\n",
    "     [1.0539542e+00, 1.1062680e-03, 1.1766142e-01],\n",
    "     [1.0000000e+00, 1.0000000e+00, 1.0000000e+00]]\n",
    ")\n",
    "\n",
    "# Test the p(z_t|x_{1:t}) calculation.\n",
    "pztt = dhmm_test.p_zt_xt(test_observations, aht, bht, ctt)\n",
    "np.testing.assert_almost_equal(\n",
    "    pztt,\n",
    "    [[7.0000000e-01, 2.0000000e-01, 1.0000000e-01],\n",
    "     [9.9999977e-01, 1.4661610e-07, 7.9086042e-08],\n",
    "     [9.4237462e-01, 5.0723353e-08, 5.7625330e-02],\n",
    "     [9.9993612e-01, 2.0604439e-05, 4.3274226e-05]]\n",
    ")\n",
    "\n",
    "# Test the p(z_t|x_{1:T}) calculation.\n",
    "pzTt = dhmm_test.p_zt_xT(test_observations, aht, bht, ctt)\n",
    "np.testing.assert_almost_equal(\n",
    "    pzTt,\n",
    "    [[9.8437499e-01, 2.4866725e-09, 1.5625005e-02],\n",
    "     [9.9999999e-01, 1.0416594e-09, 1.1953662e-08],\n",
    "     [9.9321972e-01, 5.6113622e-11, 6.7802781e-03],\n",
    "     [9.9993612e-01, 2.0604439e-05, 4.3274226e-05]]\n",
    ")\n",
    "\n",
    "# Test the log p(z_{1:T}|x_{1:T}) calculation.\n",
    "log_p_sxt = dhmm_test.log_p_sequence_xt(\n",
    "    test_observations, pzTt\n",
    ")\n",
    "np.testing.assert_almost_equal(\n",
    "    log_p_sxt, -8.1799327\n",
    ")\n",
    "\n",
    "# Test the viterbi algorithm.\n",
    "z_map_t = dhmm_test.viterbi_algorithm(test_observations)\n",
    "np.testing.assert_almost_equal(\n",
    "    z_map_t,\n",
    "    [[1., 0., 0.],\n",
    "     [1., 0., 0.],\n",
    "     [1., 0., 0.],\n",
    "     [1., 0., 0.]]\n",
    ")\n",
    "\n",
    "# Test the viterbi algorithm on a tougher problem.\n",
    "complex_observations = np.array(\n",
    "    [[1.0, -1.0, 0.2],\n",
    "     [0.6, 1.1, 1.0],\n",
    "     [1.3, 1.0, 1.2]]\n",
    " )\n",
    "z_map_t = dhmm_test.viterbi_algorithm(complex_observations)\n",
    "np.testing.assert_almost_equal(\n",
    "    z_map_t,\n",
    "    [[1., 0., 0.],\n",
    "     [1., 0., 0.],\n",
    "     [0., 0., 1.],\n",
    "     [0., 0., 1.]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Running our discrete HMM model on the observations.\n",
    "\n",
    "You've implemented everything you need, now let's run on your friend's observations. Your friend wants to compare three different latent state estimates:\n",
    "- Setting $z_t = \\text{argmax}_k \\ p(z_{t,k}|x_{1:t})$\n",
    "- Setting $z_t = \\text{argmax}_k \\ p(z_{t,k}|x_{1:T})$\n",
    "- Setting $z_t = \\text{argmax}_{z_{1:T}} \\ p(z_{1:t}|x_{1:T})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare our three options.\n",
    "dhmm = DiscreteHMM(pi = PI, transition_matrix = A_MAT)\n",
    "\n",
    "# Start with the argmax of p(z_t|x_{1:t})\n",
    "alpha_hat, c_t = dhmm.calc_alpha_hat(observations)\n",
    "beta_hat = dhmm.calc_beta_hat(observations)\n",
    "p_ztt = dhmm.p_zt_xt(observations, alpha_hat, beta_hat, c_t)\n",
    "z_max_xt = np.argmax(p_ztt, axis=-1)\n",
    "\n",
    "# Now the argmax of p(z_t|x_{1:T})\n",
    "alpha_hat, c_t = dhmm.calc_alpha_hat(observations)\n",
    "beta_hat = dhmm.calc_beta_hat(observations)\n",
    "p_ztT = dhmm.p_zt_xT(observations, alpha_hat, beta_hat, c_t)\n",
    "z_max_xT = np.argmax(p_ztT, axis=-1)\n",
    "\n",
    "# Now the argmax of log p(z_{1:t}|x_{1:T})\n",
    "z_oh_max_vt = dhmm.viterbi_algorithm(observations)\n",
    "z_max_vt = np.argmax(z_oh_max_vt, axis=-1)\n",
    "\n",
    "# Let's plot our three options\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=100)\n",
    "fontsize = 15\n",
    "\n",
    "plt.plot(z_max_xt + 3e-2, lw=4, label=r'$\\mathrm{argmax}_k \\ p(z_{t,k}|x_{1:t})$', color='#a1dab4', alpha=0.6)\n",
    "plt.plot(z_max_xT, lw=4, label=r'$\\mathrm{argmax}_k \\ p(z_{t,k}|x_{1:T})$', color='#41b6c4', alpha=0.6)\n",
    "plt.plot(z_max_vt - 3e-2, lw=4, label=r'$\\mathrm{argmax}_{z_{1:T}} \\ p(z_{1:T}|x_{1:t})$', color='#225ea8', alpha=0.6)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.ylim([-1,3])\n",
    "plt.xlim([-5,105])\n",
    "plt.xlabel('Time', fontsize=fontsize)\n",
    "plt.ylabel('State', fontsize=fontsize)\n",
    "plt.yticks([0,1,2], ['Happy','Angry','Neutral'], fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend is happy to see that all three estimates mostly agree, if a little unsettled by how much time the volunteer was angry. \n",
    "\n",
    "__Why does the volunteer spend so much time in the angry state? Make your argument using the transition matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend wants to compare the three possible sequences to find out which one is the most likely. Let's compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_z_xt = dhmm.log_p_sequence_xt(observations, p_ztt)\n",
    "prob_z_xT = dhmm.log_p_sequence_xt(observations, p_ztT)\n",
    "prob_z_vt = dhmm.log_p_sequence_xt(observations, z_oh_max_vt)\n",
    "\n",
    "print(f'Log probability of argmax_k p(z_t,k|x_1:t): {prob_z_xt}')\n",
    "print(f'Log probability of argmax_k p(z_t,k|x_1:T): {prob_z_xT}')\n",
    "print(f'Log probability of argmax_(z_1:T) p(z_t|x_1:t): {prob_z_vt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend can see that the viterbi algorithm gives the most likely sequence, but they don't understand how it's possible that $\\mathrm{argmax}_k \\ p(z_{t,k}|x_{1:t})$ has zero probability. \n",
    "\n",
    "__How is it possible for $\\mathrm{argmax}_k \\ p(z_{t,k}|x_{1:t})$ to have zero likelihood? Is it possible for the results of the viterbi algorithm to have this issue?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
